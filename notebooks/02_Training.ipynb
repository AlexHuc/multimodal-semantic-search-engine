{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9fceee6",
   "metadata": {},
   "source": [
    "# Model Training: Product Matching\n",
    "\n",
    "This notebook demonstrates how to train models for product similarity and matching tasks.\n",
    "\n",
    "**Problem**: Match duplicate/similar products across Shopee listings\n",
    "\n",
    "**Approach**: \n",
    "1. Extract text and image features\n",
    "2. Create pairs dataset (positive and negative pairs)\n",
    "3. Train similarity model (Siamese Network or Metric Learning)\n",
    "4. Evaluate on matching task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up paths\n",
    "data_dir = Path('../shopee-product-matching-data')\n",
    "train_csv = data_dir / 'train.csv'\n",
    "train_images_dir = data_dir / 'train_images'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_csv)\n",
    "print(f\"Data loaded: {train_df.shape[0]} products, {train_df['label_group'].nunique()} groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13083b",
   "metadata": {},
   "source": [
    "## 1. Create Pairs Dataset\n",
    "\n",
    "For training similarity models, we need to create pairs:\n",
    "- **Positive pairs**: Products from the same group (similar products)\n",
    "- **Negative pairs**: Products from different groups (dissimilar products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27737182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs_dataset(df, positive_ratio=0.5, seed=42):\n",
    "    \"\"\"\n",
    "    Create positive and negative pairs for training.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with products\n",
    "        positive_ratio: ratio of positive pairs (0-1)\n",
    "        seed: random seed\n",
    "    \n",
    "    Returns:\n",
    "        List of (idx1, idx2, label) tuples where label=1 for same group, 0 for different\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    pairs = []\n",
    "    \n",
    "    # Get all indices and groups\n",
    "    df_indexed = df.reset_index(drop=True)\n",
    "    group_to_indices = {}\n",
    "    \n",
    "    for idx, group_id in enumerate(df_indexed['label_group']):\n",
    "        if group_id not in group_to_indices:\n",
    "            group_to_indices[group_id] = []\n",
    "        group_to_indices[group_id].append(idx)\n",
    "    \n",
    "    # Create positive pairs (same group)\n",
    "    positive_pairs = []\n",
    "    for group_id, indices in group_to_indices.items():\n",
    "        if len(indices) >= 2:\n",
    "            # Create all pairs within group\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i+1, len(indices)):\n",
    "                    positive_pairs.append((indices[i], indices[j], 1))\n",
    "    \n",
    "    # Create negative pairs (different groups)\n",
    "    negative_pairs = []\n",
    "    num_negative = int(len(positive_pairs) / positive_ratio) - len(positive_pairs)\n",
    "    \n",
    "    all_indices = list(range(len(df_indexed)))\n",
    "    while len(negative_pairs) < num_negative:\n",
    "        idx1, idx2 = np.random.choice(all_indices, 2, replace=False)\n",
    "        if df_indexed.loc[idx1, 'label_group'] != df_indexed.loc[idx2, 'label_group']:\n",
    "            negative_pairs.append((idx1, idx2, 0))\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    pairs = positive_pairs + negative_pairs\n",
    "    np.random.shuffle(pairs)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "print(\"Creating pairs dataset...\")\n",
    "pairs = create_pairs_dataset(train_df, positive_ratio=0.5)\n",
    "\n",
    "print(f\"\\nPairs Dataset Created:\")\n",
    "print(f\"  Total pairs: {len(pairs):,}\")\n",
    "positive_count = sum(1 for p in pairs if p[2] == 1)\n",
    "negative_count = sum(1 for p in pairs if p[2] == 0)\n",
    "print(f\"  Positive pairs (same group): {positive_count:,} ({positive_count/len(pairs)*100:.1f}%)\")\n",
    "print(f\"  Negative pairs (different group): {negative_count:,} ({negative_count/len(pairs)*100:.1f}%)\")\n",
    "\n",
    "# Split into train/val\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "print(f\"\\nTrain/Val Split:\")\n",
    "print(f\"  Train pairs: {len(train_pairs):,}\")\n",
    "print(f\"  Val pairs: {len(val_pairs):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad22363",
   "metadata": {},
   "source": [
    "## 2. Dataset & DataLoader Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextImagePairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for product pairs with both text and image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, pairs, images_dir, transform=None, use_text=True, use_image=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.pairs = pairs\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.use_text = use_text\n",
    "        self.use_image = use_image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx1, idx2, label = self.pairs[idx]\n",
    "        \n",
    "        result = {'label': torch.tensor(label, dtype=torch.float32)}\n",
    "        \n",
    "        # Get text features\n",
    "        if self.use_text:\n",
    "            title1 = self.df.loc[idx1, 'title']\n",
    "            title2 = self.df.loc[idx2, 'title']\n",
    "            \n",
    "            # Simple text encoding: character count and word count\n",
    "            text_features1 = np.array([\n",
    "                len(title1),\n",
    "                len(title1.split())\n",
    "            ], dtype=np.float32)\n",
    "            text_features2 = np.array([\n",
    "                len(title2),\n",
    "                len(title2.split())\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            result['text1'] = torch.tensor(text_features1)\n",
    "            result['text2'] = torch.tensor(text_features2)\n",
    "        \n",
    "        # Get image features\n",
    "        if self.use_image:\n",
    "            img_path1 = self.images_dir / self.df.loc[idx1, 'image']\n",
    "            img_path2 = self.images_dir / self.df.loc[idx2, 'image']\n",
    "            \n",
    "            try:\n",
    "                img1 = Image.open(img_path1).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img1 = self.transform(img1)\n",
    "                result['image1'] = img1\n",
    "            except:\n",
    "                result['image1'] = torch.zeros(3, 224, 224)\n",
    "            \n",
    "            try:\n",
    "                img2 = Image.open(img_path2).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img2 = self.transform(img2)\n",
    "                result['image2'] = img2\n",
    "            except:\n",
    "                result['image2'] = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextImagePairDataset(\n",
    "    train_df, train_pairs, train_images_dir, \n",
    "    transform=image_transform, use_text=True, use_image=True\n",
    ")\n",
    "\n",
    "val_dataset = TextImagePairDataset(\n",
    "    train_df, val_pairs, train_images_dir,\n",
    "    transform=image_transform, use_text=True, use_image=True\n",
    ")\n",
    "\n",
    "print(f\"Train Dataset: {len(train_dataset)} pairs\")\n",
    "print(f\"Val Dataset: {len(val_dataset)} pairs\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch_size={batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fcb72",
   "metadata": {},
   "source": [
    "## 3. Model Architecture: Siamese Network\n",
    "\n",
    "A Siamese network learns to compare two inputs and output a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese Network for product similarity.\n",
    "    Combines image embeddings and text embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_embedding_dim=512, text_embedding_dim=32, fusion_dim=256):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Image encoder (using pre-trained ResNet50)\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])  # Remove classification layer\n",
    "        self.image_fc = nn.Linear(2048, image_embedding_dim)\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, text_embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Fusion layers\n",
    "        total_embedding_dim = image_embedding_dim + text_embedding_dim\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(total_embedding_dim * 2, fusion_dim),  # *2 because we concatenate both pairs\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Output probability\n",
    "        )\n",
    "    \n",
    "    def encode_pair(self, images, texts):\n",
    "        \"\"\"\n",
    "        Encode image and text pair into embeddings.\n",
    "        \"\"\"\n",
    "        # Image encoding\n",
    "        img_features = self.image_encoder(images)\n",
    "        img_features = img_features.view(img_features.size(0), -1)\n",
    "        img_embedding = self.image_fc(img_features)\n",
    "        \n",
    "        # Text encoding\n",
    "        text_embedding = self.text_encoder(texts)\n",
    "        \n",
    "        # Concatenate\n",
    "        embedding = torch.cat([img_embedding, text_embedding], dim=1)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, image1, text1, image2, text2):\n",
    "        \"\"\"\n",
    "        Forward pass: compare two product pairs.\n",
    "        \"\"\"\n",
    "        # Encode both products\n",
    "        embedding1 = self.encode_pair(image1, text1)\n",
    "        embedding2 = self.encode_pair(image2, text2)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        combined = torch.cat([embedding1, embedding2], dim=1)\n",
    "        \n",
    "        # Predict similarity\n",
    "        similarity = self.fusion(combined)\n",
    "        \n",
    "        return similarity.squeeze()\n",
    "\n",
    "# Initialize model\n",
    "model = SiameseNetwork(\n",
    "    image_embedding_dim=512,\n",
    "    text_embedding_dim=32,\n",
    "    fusion_dim=256\n",
    ").to(device)\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nModel initialized on device: {device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07046eea",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for similarity prediction\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        image1 = batch['image1'].to(device)\n",
    "        image2 = batch['image2'].to(device)\n",
    "        text1 = batch['text1'].to(device)\n",
    "        text2 = batch['text2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image1, text1, image2, text2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions.extend(outputs.detach().cpu().numpy())\n",
    "        targets.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    auc = roc_auc_score(targets, predictions)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            image1 = batch['image1'].to(device)\n",
    "            image2 = batch['image2'].to(device)\n",
    "            text1 = batch['text1'].to(device)\n",
    "            text2 = batch['text2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(image1, text1, image2, text2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    auc = roc_auc_score(targets, predictions)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "print(\"Training setup complete. Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eecdc82",
   "metadata": {},
   "source": [
    "## 5. Training Execution\n",
    "\n",
    "**Note**: Full training may take 10-30 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 10\n",
    "best_val_auc = 0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_aucs = []\n",
    "val_aucs = []\n",
    "\n",
    "print(f\"\\nStarting training for {epochs} epochs...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_auc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_aucs.append(train_auc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_auc = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_aucs.append(val_auc)\n",
    "    \n",
    "    print(f\"\\nTraining Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"âœ“ Best model saved! (AUC: {val_auc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Validation AUC: {best_val_auc:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7fe85f",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ae564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[1].plot(train_aucs, label='Train AUC', marker='o', linewidth=2)\n",
    "axes[1].plot(val_aucs, label='Val AUC', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('AUC Score')\n",
    "axes[1].set_title('Training & Validation AUC', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  Final Train AUC: {train_aucs[-1]:.4f}\")\n",
    "print(f\"  Final Val AUC: {val_aucs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8739972",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Get predictions on validation set\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        image1 = batch['image1'].to(device)\n",
    "        image2 = batch['image2'].to(device)\n",
    "        text1 = batch['text1'].to(device)\n",
    "        text2 = batch['text2'].to(device)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = model(image1, text1, image2, text2)\n",
    "        all_predictions.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(labels.numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Convert to binary predictions (threshold = 0.5)\n",
    "binary_predictions = (all_predictions >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(f\"\\nEvaluation Metrics on Validation Set:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nPrecision: {precision_score(all_targets, binary_predictions):.4f}\")\n",
    "print(f\"Recall: {recall_score(all_targets, binary_predictions):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(all_targets, binary_predictions):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(all_targets, all_predictions):.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, binary_predictions, \n",
    "                          target_names=['Different Product', 'Same Product']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_targets, binary_predictions)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "print(f\"  FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Different', 'Same'],\n",
    "            yticklabels=['Different', 'Same'])\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Prediction distribution\n",
    "axes[1].hist(all_predictions[all_targets == 0], bins=30, alpha=0.6, label='Different (Label=0)', color='red')\n",
    "axes[1].hist(all_predictions[all_targets == 1], bins=30, alpha=0.6, label='Same (Label=1)', color='green')\n",
    "axes[1].axvline(0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "axes[1].set_xlabel('Predicted Similarity Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Prediction Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165378ab",
   "metadata": {},
   "source": [
    "## 8. Save Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "model_path = 'shopee_siamese_model.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_architecture': 'SiameseNetwork',\n",
    "    'hyperparameters': {\n",
    "        'image_embedding_dim': 512,\n",
    "        'text_embedding_dim': 32,\n",
    "        'fusion_dim': 256\n",
    "    },\n",
    "    'training_metrics': {\n",
    "        'best_val_auc': best_val_auc,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1]\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"âœ“ Model saved to {model_path}\")\n",
    "print(f\"\\nModel can be loaded with:\")\n",
    "print(f\"  checkpoint = torch.load('{model_path}')\")\n",
    "print(f\"  model.load_state_dict(checkpoint['model_state_dict'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6597ee",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"MODEL TRAINING COMPLETE - SUMMARY\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š ARCHITECTURE\")\n",
    "print(f\"  - Base: Siamese Network with multi-modal fusion\")\n",
    "print(f\"  - Image encoder: ResNet50 (pre-trained)\")\n",
    "print(f\"  - Text encoder: 2-layer MLP\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ TRAINING RESULTS\")\n",
    "print(f\"  - Best Validation AUC: {best_val_auc:.4f}\")\n",
    "print(f\"  - Precision: {precision_score(all_targets, binary_predictions):.4f}\")\n",
    "print(f\"  - Recall: {recall_score(all_targets, binary_predictions):.4f}\")\n",
    "print(f\"  - F1-Score: {f1_score(all_targets, binary_predictions):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ MODEL SAVED\")\n",
    "print(f\"  - Location: {model_path}\")\n",
    "print(f\"  - Use for inference on new product pairs\")\n",
    "\n",
    "print(f\"\\nðŸš€ NEXT STEPS\")\n",
    "print(f\"  1. Deploy model for inference\")\n",
    "print(f\"  2. Fine-tune with harder negative mining\")\n",
    "print(f\"  3. Experiment with different architectures (Transformer, CLIP)\")\n",
    "print(f\"  4. Test on competition test set\")\n",
    "print(f\"  5. Ensemble with multiple models for better performance\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
